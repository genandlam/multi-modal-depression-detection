{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Topic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckjoshi9/miniconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "transcripts = pd.read_csv(\"transcripts_topic.tsv\", sep=\"\\t\")\n",
    "scores_train = pd.read_csv(\"train_split_Depression_AVEC2017.csv\")\n",
    "scores_dev = pd.read_csv(\"dev_split_Depression_AVEC2017.csv\")\n",
    "scores_test = pd.read_csv(\"test_split_Depression_AVEC2017.csv\")\n",
    "scores = pd.concat([scores_train, scores_dev, scores_test])\n",
    "scores = scores.set_index(\"Participant_ID\")\n",
    "scores = scores[\"PHQ8_Binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>speaker</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_value</th>\n",
       "      <th>sub_topic</th>\n",
       "      <th>participant</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.236</td>\n",
       "      <td>174.446</td>\n",
       "      <td>Participant</td>\n",
       "      <td>what do you mean i'm sorry</td>\n",
       "      <td>4.0</td>\n",
       "      <td>do you consider yourself an introvert</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>756.786</td>\n",
       "      <td>757.876</td>\n",
       "      <td>Participant</td>\n",
       "      <td>oh wow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>how easy is it for you to get a good night's s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>759.366</td>\n",
       "      <td>761.846</td>\n",
       "      <td>Participant</td>\n",
       "      <td>i have my days um</td>\n",
       "      <td>1.0</td>\n",
       "      <td>how easy is it for you to get a good night's s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>816.806</td>\n",
       "      <td>821.326</td>\n",
       "      <td>Participant</td>\n",
       "      <td>what am i like irritated tired um lazy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>what are you like when you don't sleep well</td>\n",
       "      <td>1.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>822.486</td>\n",
       "      <td>823.416</td>\n",
       "      <td>Participant</td>\n",
       "      <td>you know</td>\n",
       "      <td>1.0</td>\n",
       "      <td>what are you like when you don't sleep well</td>\n",
       "      <td>1.0</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  stop_time      speaker                                   value  \\\n",
       "0     173.236    174.446  Participant              what do you mean i'm sorry   \n",
       "1     756.786    757.876  Participant                                  oh wow   \n",
       "2     759.366    761.846  Participant                       i have my days um   \n",
       "3     816.806    821.326  Participant  what am i like irritated tired um lazy   \n",
       "4     822.486    823.416  Participant                                you know   \n",
       "\n",
       "   topic                                        topic_value  sub_topic  \\\n",
       "0    4.0              do you consider yourself an introvert        0.0   \n",
       "1    1.0  how easy is it for you to get a good night's s...        0.0   \n",
       "2    1.0  how easy is it for you to get a good night's s...        0.0   \n",
       "3    1.0        what are you like when you don't sleep well        1.0   \n",
       "4    1.0        what are you like when you don't sleep well        1.0   \n",
       "\n",
       "   participant  Gender  \n",
       "0          303       0  \n",
       "1          303       0  \n",
       "2          303       0  \n",
       "3          303       0  \n",
       "4          303       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Participant_ID\n",
       "303    0.0\n",
       "304    0.0\n",
       "305    0.0\n",
       "310    0.0\n",
       "312    0.0\n",
       "Name: PHQ8_Binary, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual dictionary for topic/subtopic special tokens for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_subtopic_to_category = {\n",
    "    0 : {\n",
    "        0 : \"did_recently\",\n",
    "        1 : \"enjoy_travelling\",\n",
    "        3 : \"family_relationship\",\n",
    "        4 : \"do_for_fun\",\n",
    "        5 : \"best_friend\",\n",
    "        6 : \"ideal_weekend\"\n",
    "    },\n",
    "    \n",
    "    1 : {\n",
    "        0 : \"easy_sleep\",\n",
    "        1 : \"sleep_badly\"\n",
    "    },\n",
    "    \n",
    "    2 : {\n",
    "        0 : \"happy_last_time\",\n",
    "        1 : \"behaviour_changes\",\n",
    "        2 : \"disturbing_thoughts\",\n",
    "        3 : \"feel_lately\"\n",
    "    },\n",
    "    \n",
    "    3 : {\n",
    "        0 : \"any_regret\",\n",
    "        1 : \"feel_guilty\",\n",
    "        2 : \"most_proud\",\n",
    "    },\n",
    "    \n",
    "    4 : {\n",
    "        0 : \"introvert\",\n",
    "        1 : \"shy_outgoing\",\n",
    "    },\n",
    "    \n",
    "    5 : {\n",
    "        0 : \"ptsd_diagnosed\",\n",
    "        1 : \"depression_diagnosed\",\n",
    "        2 : \"therapy_useful\",\n",
    "        \n",
    "    },\n",
    "    \n",
    "    6 : {\n",
    "        2 : \"easy_parent\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pariticipant ID to text dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO can add preprocessing steps here\n",
    "def preprocess(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_to_text = {}\n",
    "\n",
    "prev_topic = \"\"\n",
    "prev_subtopic = \"\"\n",
    "prev_participant = -1\n",
    "\n",
    "for idx, row in transcripts.iterrows():\n",
    "    participant = row.participant\n",
    "    topic = int(row.topic)\n",
    "    subtopic = int(row.sub_topic)\n",
    "    text = row.value\n",
    "    \n",
    "    if participant not in participant_to_text:\n",
    "        # Create blank entry for new participant\n",
    "        participant_to_text[participant] = [\"\", []]\n",
    "    \n",
    "    if participant == prev_participant and topic == prev_topic and subtopic == prev_subtopic:\n",
    "        # If previous participant and topic+subtopic, don't pad special tokens\n",
    "        proc_text = preprocess(text) + \" \"\n",
    "        participant_to_text[participant][0] += proc_text\n",
    "        participant_to_text[participant][1][-1] += proc_text\n",
    "    else:\n",
    "        # If different topic+subtopic, pad special token infront of text before appending to full text and topic-wise text\n",
    "        proc_text = topic_to_subtopic_to_category[topic][subtopic] + \" \" + preprocess(text) + \" \"\n",
    "        participant_to_text[participant][0] += proc_text\n",
    "        participant_to_text[participant][1].append(proc_text)\n",
    "        \n",
    "    prev_participant = participant\n",
    "    prev_topic = topic\n",
    "    prev_subtopic = subtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text (all topics): \n",
      "introvert what do you mean i'm sorry easy_sleep oh wow i have my days um sleep_badly what am i like irritated tired um lazy you know depression_diagnosed no best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \n",
      "\n",
      "Topic-wise text: \n",
      "[\"introvert what do you mean i'm sorry \", 'easy_sleep oh wow i have my days um ', 'sleep_badly what am i like irritated tired um lazy you know ', 'depression_diagnosed no ', \"best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible \", \"happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \"]\n"
     ]
    }
   ],
   "source": [
    "# Example output\n",
    "print(\"Full text (all topics): \")\n",
    "print(participant_to_text[303][0])\n",
    "print(\"\\nTopic-wise text: \")\n",
    "print(participant_to_text[303][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test data\n",
    "- Currently, use dev set data as Test data and training set data as Training data\n",
    "- **IMPORTANT**: For training data, we are doing data augmentation as follows:\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation parameters \n",
    "# Here, 0-> non-depressed class, 1-> depressed class\n",
    "# We treat the two classes differently (we do more augmentation for depressed class)\n",
    "min_len = {0: 5, 1: 3}  # Minimum length of a transcript above which we can do augmentation \n",
    "aug_count = {0: 5, 1: 15} # Number of augmented transcripts to be created\n",
    "\n",
    "data_train = {\"Text\": [], \"Targets\": []}\n",
    "data_test = {\"Text\": [], \"Targets\": []}\n",
    "\n",
    "for participant in participant_to_text:\n",
    "    # Training data\n",
    "    if participant in scores_train[\"Participant_ID\"].values:\n",
    "        # Add un-augmented transcript\n",
    "        data_train[\"Text\"].append(participant_to_text[participant][0])\n",
    "        data_train[\"Targets\"].append(scores[participant])\n",
    "        \n",
    "        # Data augmentation step (only for those transcripts which are longer than min_len)\n",
    "        if len(participant_to_text[participant][1]) > min_len[scores[participant]]:\n",
    "            # Generate aug_count integers, each of which is the length of the new transcript \n",
    "            # (each entry in t_len is in range min_len to size of current transcript)\n",
    "            t_lens = np.random.randint(low=min_len[scores[participant]], \n",
    "                                       high=len(participant_to_text[participant][1]), \n",
    "                                       size=aug_count[scores[participant]])\n",
    "            for t_len in t_lens:\n",
    "                # Generate list of all combinations of topic texts of t_len\n",
    "                combs = list(combinations(participant_to_text[participant][1], t_len))\n",
    "                # Select a random combination\n",
    "                t_comb = list(combs[np.random.randint(len(combs))])\n",
    "                # Shuffle the topic texts in selected combination\n",
    "                np.random.shuffle(t_comb)\n",
    "                # Add augmented transcript\n",
    "                data_train[\"Text\"].append(\" \".join(t_comb))\n",
    "                data_train[\"Targets\"].append(scores[participant])\n",
    "\n",
    "    # Testing data\n",
    "    elif participant in scores_dev[\"Participant_ID\"].values:\n",
    "        data_test[\"Text\"].append(participant_to_text[participant][0])\n",
    "        data_test[\"Targets\"].append(scores[participant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([462, 480]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance in training data\n",
    "np.unique(data_train[\"Targets\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"introvert what do you mean i'm sorry easy_sleep oh wow i have my days um sleep_badly what am i like irritated tired um lazy you know depression_diagnosed no best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \",\n",
       " \"sleep_badly what am i like irritated tired um lazy you know  happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean  depression_diagnosed no  best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible  introvert what do you mean i'm sorry \",\n",
       " \"introvert what do you mean i'm sorry  sleep_badly what am i like irritated tired um lazy you know  depression_diagnosed no  happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean  best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible \",\n",
       " \"sleep_badly what am i like irritated tired um lazy you know  happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean  easy_sleep oh wow i have my days um  introvert what do you mean i'm sorry  depression_diagnosed no \",\n",
       " \"easy_sleep oh wow i have my days um  happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean  introvert what do you mean i'm sorry  sleep_badly what am i like irritated tired um lazy you know  best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible \",\n",
       " \"introvert what do you mean i'm sorry  depression_diagnosed no  sleep_badly what am i like irritated tired um lazy you know  easy_sleep oh wow i have my days um  happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \",\n",
       " \"family_relationship very close even though i don't live with them i try to see them as much as i can introvert mm yes  enjoy_travelling um trying new things seeing new views of the world um trying the different type of foods um seeing how the government is and how they run the things out there i guess easy_sleep it's pretty good eh somewhat sleep_badly i'm tired <laughter> and i kind of fall asleep during class and whatnot depression_diagnosed no best_friend very friendly and funny talkative happy_last_time um last weekend i guess \",\n",
       " \"family_relationship very close even though i don't live with them i try to see them as much as i can  happy_last_time um last weekend i guess  best_friend very friendly and funny talkative  introvert mm yes   easy_sleep it's pretty good eh somewhat  sleep_badly i'm tired <laughter> and i kind of fall asleep during class and whatnot \",\n",
       " \"family_relationship very close even though i don't live with them i try to see them as much as i can  happy_last_time um last weekend i guess  sleep_badly i'm tired <laughter> and i kind of fall asleep during class and whatnot  easy_sleep it's pretty good eh somewhat  best_friend very friendly and funny talkative  introvert mm yes   enjoy_travelling um trying new things seeing new views of the world um trying the different type of foods um seeing how the government is and how they run the things out there i guess \",\n",
       " \"introvert mm yes   sleep_badly i'm tired <laughter> and i kind of fall asleep during class and whatnot  easy_sleep it's pretty good eh somewhat  enjoy_travelling um trying new things seeing new views of the world um trying the different type of foods um seeing how the government is and how they run the things out there i guess  depression_diagnosed no \"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_train).to_csv(\"data_train_aug.csv\", index=False)\n",
    "pd.DataFrame(data_test).to_csv(\"data_test_aug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words are there in training and testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720, 823)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = \" \".join(data_train[\"Text\"])\n",
    "train_text = np.unique(train_text.split(\" \"))\n",
    "test_text = \" \".join(data_test[\"Text\"])\n",
    "test_text = np.unique(test_text.split(\" \"))\n",
    "len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words in testing data are not found in training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for w in test_text:\n",
    "    if w not in train_text:\n",
    "        x.append(w)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Topic-modelling without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {\"Text\": [], \"Targets\": []}\n",
    "data_test = {\"Text\": [], \"Targets\": []}\n",
    "\n",
    "for participant in participant_to_text:\n",
    "    # Training data\n",
    "    if participant in scores_train[\"Participant_ID\"].values:\n",
    "        # Add un-augmented transcript\n",
    "        data_train[\"Text\"].append(participant_to_text[participant][0])\n",
    "        data_train[\"Targets\"].append(scores[participant])\n",
    "        \n",
    "    # Testing data\n",
    "    elif participant in scores_dev[\"Participant_ID\"].values:\n",
    "        data_test[\"Text\"].append(participant_to_text[participant][0])\n",
    "        data_test[\"Targets\"].append(scores[participant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([77, 30]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance in training data\n",
    "np.unique(data_train[\"Targets\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"introvert what do you mean i'm sorry easy_sleep oh wow i have my days um sleep_badly what am i like irritated tired um lazy you know depression_diagnosed no best_friend i don't really have a best friend but a person that i deal with and i used to work with um she would probably tell you that i'm very um outgoing a go getter dependable responsible happy_last_time well i try to stay happy i'd rather be happy than sad my kids keep me going you know what i mean \",\n",
       " \"family_relationship very close even though i don't live with them i try to see them as much as i can introvert mm yes  enjoy_travelling um trying new things seeing new views of the world um trying the different type of foods um seeing how the government is and how they run the things out there i guess easy_sleep it's pretty good eh somewhat sleep_badly i'm tired <laughter> and i kind of fall asleep during class and whatnot depression_diagnosed no best_friend very friendly and funny talkative happy_last_time um last weekend i guess \",\n",
       " \"do_for_fun uh fun i like going to the beach uh family_relationship not uh i don't have much family as it is easy_sleep um it's been hard lately it's been probably hard for the last uh going on a year um sleep_badly um tired <laughter> therapy_useful pardon me i still didn't hear you that people have been deceitful depression_diagnosed uh i've been diagnosed with uh bipolarism best_friend how does my friends describe me \",\n",
       " \"enjoy_travelling uh i like traveling by train it's not my favorite thing introvert oh yeah sure absolutely yeah  family_relationship not at all depression_diagnosed <clears throat> no behaviour_changes i'm sorry you repeat that no not necessarily <clears throat> easy_sleep very i'm a heavy sleeper happy_last_time <sigh> uh most_proud most proud of did_recently uh spent new year's eve with a friend real close friend so any_regret in terms of what best_friend oh i don't have a best friend these days <laughter> \",\n",
       " \"enjoy_travelling i don't i don't enjoy traveling family_relationship fairly close um i see them pretty  introvert yes best_friend i don't really have a best friend introverted remember most_proud right now it's the fact that happy_last_time um a few weeks ago when i got a good grade in a class depression_diagnosed nope easy_sleep it depends if i workout really hard the night before <bef> the day the day before if workout pretty hard it's bad but if i don't workout it i don't know just eh it's hard sleep_badly i'm probably like i am now normal i think not as not as happy about everything  but still pretty okay i can still function behaviour_changes no maybe slightly any_regret no not really \",\n",
       " \"do_for_fun i like to go to the gym uh hang out with friends read introvert mm at times happy_last_time hmm uh maybe like behaviour_changes uh yeah i'm i'm easier more easily irritated depression_diagnosed uh yeah awhile ago most_proud uh being able to any_regret yeah lots of stuff best_friend um talented \",\n",
       " \"enjoy_travelling um just the independence about it being alone being able to be free and not tied down to one to one thing you know it's great introvert um yeah uh a little bit family_relationship um i'm pretty close um i'm pretty close with my family do_for_fun um i definitely like to watch sports uh i i like to gamble i i like to travel i like to skate depression_diagnosed no easy_sleep um it's okay i'd say it's pretty easy sleep_badly irritated tired behaviour_changes huh what did you say no not really best_friend um i think he would describe me as a stand up guy i try to be um um a good loyal friend and and an adult i think he would describe me like that  any_regret um yes a lot of things i regret um i regret most_proud um being a positive role model right now um for younger kids that's in um our sports academy program i i think as i got older happy_last_time oh um couple days ago my football team um we're in the playoffs made the playoffs so that always makes me happy \",\n",
       " \"family_relationship <laughter> very introvert mm no do_for_fun mm i like swimming i like museums i like plays i like concerts enjoy_travelling uh s being in a different environment learning different cultures meeting people  depression_diagnosed yes behaviour_changes yes uh i i sleep a lot more  happy_last_time when my uh nephew called me scrubbed_entry most_proud uh that i i'm a pretty good person caring person did_recently xxx i enjoyed best_friend mm as a good caring friend and somebody who likes to do things any_regret uh not going farther in my education most_proud uh that i've <clears throat> been a very  kind considerate helpful person \",\n",
       " \"family_relationship do you travel a lot  um just locally travel since i've been here  enjoy_travelling um  a change of pace new environment and um relaxation and escape from  introvert um i'd yes i'd say yes  most of the time  depression_diagnosed yes best_friend <cough> um  i guess easy-going and um likes to have fun sometimes   any_regret uh regret yes yeah  happy_last_time um  i'd say maybe ten <te> ten years ago or so  most_proud um i'd say finishing grad school and finishing um some \",\n",
       " \"introvert definitely  enjoy_travelling just being able to see <s> to get a change of pace see somebody new see new faces new environment  depression_diagnosed no easy_sleep uh lately it's been pretty tough i guess because i've been staying up so late but uh that's my fault <laughter> sleep_badly sluggish and tired all day if i don't sleep well all day i'm just laying around  behaviour_changes no not too much  any_regret mm no <laughter> best_friend um hilarious family_relationship pretty close  did_recently mm recently what did i do um new year's eve party i had i had the time of my life it was so fun most_proud mm i don't know that's a tough one to to answer what am i most proud of  i don't know honestly  \"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_train).to_csv(\"data_train_topic.csv\", index=False)\n",
    "pd.DataFrame(data_test).to_csv(\"data_test_topic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Full transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = pd.read_csv(\"train_split_Depression_AVEC2017.csv\").set_index(\"Participant_ID\")\n",
    "scores_dev = pd.read_csv(\"dev_split_Depression_AVEC2017.csv\").set_index(\"Participant_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    text = str(text)\n",
    "    return text\n",
    "\n",
    "def get_text(participant_id):\n",
    "    transcript = pd.read_csv(\"transcripts/{}_TRANSCRIPT.csv\".format(participant_id), delimiter=\"\\t\")\n",
    "    text = \"\"\n",
    "    for idx, row in transcript.iterrows():\n",
    "        if row[\"speaker\"] == \"Participant\":\n",
    "            text += \" \" + process(row[\"value\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {\"Text\": [], \"Targets\": []}\n",
    "data_test = {\"Text\": [], \"Targets\": []}\n",
    "\n",
    "for participant_id, scores in scores_train.iterrows():\n",
    "    data_train[\"Text\"].append(get_text(participant_id))\n",
    "    data_train[\"Targets\"].append(scores[\"PHQ8_Binary\"])\n",
    "    \n",
    "for participant_id, scores in scores_dev.iterrows():\n",
    "    data_test[\"Text\"].append(get_text(participant_id))\n",
    "    data_test[\"Targets\"].append(scores[\"PHQ8_Binary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_train).to_csv(\"data_train_full.csv\", index=False)\n",
    "pd.DataFrame(data_test).to_csv(\"data_test_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
